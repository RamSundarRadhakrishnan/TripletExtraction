{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d4165d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f51f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6622636",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"google/gemma-3-4b-it\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, \n",
    "    bnb_4bit_quant_type=\"nf4\", \n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    )\n",
    "#llm_int8_enable_fp32_cpu_offload=True,\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name, \n",
    "    token=\"\"\n",
    "    )\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    quantization_config=bnb_config, \n",
    "    torch_dtype=torch.bfloat16,\n",
    "    ).cuda()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c3fb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    batch_size=64  \n",
    ")\n",
    "def extract_keywords_batch(sentences, max_new_tokens=64):\n",
    "    prompts = [\n",
    "        f'Find the important keywords or entities in the following sentence:\\n'\n",
    "        f'Sentence: \"{s}\"\\nKeywords:'\n",
    "        for s in sentences\n",
    "    ]\n",
    "    outputs = generator(prompts, max_new_tokens=max_new_tokens, do_sample=False)\n",
    "    batch_keywords = []\n",
    "    for group in outputs:\n",
    "        # group is a list of generated candidates; take the first\n",
    "        text = group[0]['generated_text']\n",
    "        kws = text.split(\"Keywords:\")[-1].strip().split(\",\")\n",
    "        batch_keywords.append([k.strip() for k in kws if k.strip()])\n",
    "    return batch_keywords\n",
    "\n",
    "def generate_triplets_batch(keywords_list, sentences, max_new_tokens=128):\n",
    "    prompts = [\n",
    "        f'Given the keywords: {\", \".join(kw)}\\n'\n",
    "        f'And the sentence: \"{s}\"\\n'\n",
    "        'Identify factual relationships between the keywords in the form:\\n'\n",
    "        '(subject; relation; object)\\nTriplets:'\n",
    "        for kw, s in zip(keywords_list, sentences)\n",
    "    ]\n",
    "    outputs = generator(prompts, max_new_tokens=max_new_tokens, do_sample=False)\n",
    "    batch_triplets = []\n",
    "    for group in outputs:\n",
    "        text = group[0]['generated_text']\n",
    "        lines = text.split(\"\\n\")\n",
    "        raw = [l for l in lines if l.startswith(\"(\")]\n",
    "        # dedupe\n",
    "        seen = set(); clean = []\n",
    "        for line in raw:\n",
    "            inner = line.strip(\"() \")\n",
    "            parts = re.split(r\"\\s*[;,]\\s*\", inner)\n",
    "            if len(parts) >= 3:\n",
    "                key = tuple(p.strip() for p in parts[:3])\n",
    "                if key not in seen:\n",
    "                    seen.add(key)\n",
    "                    clean.append(f\"({key[0]}; {key[1]}; {key[2]})\")\n",
    "        batch_triplets.append(clean)\n",
    "    return batch_triplets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e73dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"INTRODUCTION A modern computer consists of one or more processors, some main memory, disks, printers, a keyboard, a mouse, a display, network interfaces, and various other input/output devices.\"\n",
    "triplets = process_sentence(sentence)\n",
    "\n",
    "print(\"Final Triplets for KG:\")\n",
    "for t in triplets:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4161320",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import logging\n",
    "\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "def parse_triplet_str(triplet_str):\n",
    "    inner = triplet_str.strip(\"() \")\n",
    "    parts = re.split(r\"\\s*;\\s*\", inner)\n",
    "    return tuple(parts) if len(parts) == 3 else None\n",
    "\n",
    "df = pd.read_csv(\"sentences50k.csv\")\n",
    "triplet_data = []\n",
    "batch_size = 64\n",
    "\n",
    "for i in tqdm(range(0, len(df), batch_size), desc=\"Extracting triplets\"):\n",
    "    sentences_batch = df[\"sentences\"].iloc[i:i+batch_size].tolist()\n",
    "    keywords_batch = extract_keywords_batch(sentences_batch)\n",
    "    triplets_batch = generate_triplets_batch(keywords_batch, sentences_batch)\n",
    "    \n",
    "    for sent, triplets in zip(sentences_batch, triplets_batch):\n",
    "        for t in triplets:\n",
    "            subj, rel, obj = re.split(r\"\\s*[;,]\\s*\", t.strip(\"() \"))\n",
    "            triplet_data.append({\n",
    "                \"subject\": subj,\n",
    "                \"relation\": rel,\n",
    "                \"object\": obj\n",
    "            })\n",
    "\n",
    "pd.DataFrame(triplet_data).to_csv(\"triplets_50k.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43afa1cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
